# -*- coding: utf-8 -*-
"""app.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1e6L4QzbDaE3yFUPmE49J1GCiRxFaDKiP
"""

from flask import Flask, request, jsonify, send_file
import joblib
import pandas as pd
import io

app = Flask(__name__)

# ============================================================
# 1) Load the trained model and scaler
# ============================================================
try:
    modelo = joblib.load("final_model.pkl")
    scaler = joblib.load("scaler.pkl")
    print("✅ Model and scaler loaded successfully")
except Exception as e:
    print(f"❌ Error loading model or scaler: {e}")

# Define the same columns used in training
categorical_columns = ['Income_type', 'Education_type', 'Family_status',
                       'Housing_type', 'Occupation_type']
numerical_features = ['Account_length', 'Total_income', 'Age', 'Years_employed']

@app.route("/", methods=["GET"])
def home():
    return "✅ Prediction API is running on Render!"

# ============================================================
# 2) Endpoint for single-instance prediction
# ============================================================
@app.route("/predict", methods=["POST"])
def predict():
    """
    Receives a JSON object with one instance's data and returns the prediction.
    """
    if not request.is_json:
        return jsonify({"error": "Content must be in JSON format"}), 400

    data = request.get_json()
    df = pd.DataFrame([data])

    # Apply One-Hot Encoding
    df = pd.get_dummies(df, columns=categorical_columns, drop_first=False)

    # Ensure the data has the same columns as in training
    missing_cols = set(scaler.feature_names_in_) - set(df.columns)
    for col in missing_cols:
        df[col] = 0  # Fill missing columns with 0

    df = df[scaler.feature_names_in_]  # Arrange columns in the same order

    # Apply MinMax Scaling
    df[numerical_features] = scaler.transform(df[numerical_features])

    # Make the prediction
    prediction = modelo.predict(df)[0]

    return jsonify({"prediction": int(prediction)})

# ============================================================
# 3) Endpoint for batch prediction via CSV
# ============================================================
@app.route("/predict_csv", methods=["POST"])
def predict_csv():
    """
    Receives a CSV file and returns the same CSV with a "Target" column containing predictions.
    """
    if "file" not in request.files:
        return jsonify({"error": "No file provided with key 'file'"}), 400

    file = request.files["file"]
    df = pd.read_csv(file)

    # Apply One-Hot Encoding
    df = pd.get_dummies(df, columns=categorical_columns, drop_first=False)

    # Ensure the data has the same columns as in training
    missing_cols = set(scaler.feature_names_in_) - set(df.columns)
    for col in missing_cols:
        df[col] = 0  # Fill missing columns with 0

    df = df[scaler.feature_names_in_]  # Arrange columns in the same order

    # Apply MinMax Scaling
    df[numerical_features] = scaler.transform(df[numerical_features])

    # Make predictions
    df["Target"] = modelo.predict(df)

    # Convert to CSV format
    output = io.StringIO()
    df.to_csv(output, index=False)
    output.seek(0)

    return send_file(
        io.BytesIO(output.getvalue().encode()),
        mimetype="text/csv",
        as_attachment=True,
        attachment_filename="predictions.csv"
    )

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=10000)  # Run on port 10000 (compatible with Render)
